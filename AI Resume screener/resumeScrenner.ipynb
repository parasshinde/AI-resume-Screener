{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# NLP and embedding tools\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from transformers import AutoTokenizer\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#Document processing\n",
    "import PyPDF2\n",
    "from pdfminer.high_level import extract_text\n",
    "from docx import document\n",
    "\n",
    "#visulization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#utilities\n",
    "import os\n",
    "import  re \n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import faiss\n",
    "\n",
    "#machine learning tools\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully logged into Hugging Face!\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login\n",
    "load_dotenv()\n",
    "hf_token = os.getenv(\"HUGGINGFACE_API_TOKEN\")\n",
    "\n",
    "if hf_token:\n",
    "    login(token=hf_token)\n",
    "    print(\"✅ Successfully logged into Hugging Face!\")\n",
    "else:\n",
    "    print(\"❌ Hugging Face API token not found. Check your .env file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\shind\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\shind\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#embedding model\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\", token=hf_token)\n",
    "\n",
    "#nltk resources\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "#spacy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 853/853 [00:00<00:00, 3257.42 examples/s]\n"
     ]
    }
   ],
   "source": [
    "#loading the Job Description dataset\n",
    "from datasets import load_dataset # type: ignore\n",
    "dataset = load_dataset(\"jacob-hugging-face/job-descriptions\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['company_name', 'job_description', 'position_title', 'description_length', 'model_response'],\n",
       "        num_rows: 853\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing dataset: 100%|██████████| 853/853 [00:01<00:00, 635.47 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\", token = hf_token)\n",
    "\n",
    "def tokenize_func(examples):\n",
    "    return tokenizer(examples[\"job_description\"], padding= \"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_func,\n",
    "    batched = True,\n",
    "    desc = \"Tokenizing dataset\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:02<00:00,  2.67s/it]? examples/s]\n",
      "Batches: 100%|██████████| 1/1 [00:02<00:00,  2.50s/it]:13, 11.18 examples/s]\n",
      "Batches: 100%|██████████| 1/1 [00:02<00:00,  2.51s/it]:05, 12.00 examples/s]\n",
      "Batches: 100%|██████████| 1/1 [00:02<00:00,  2.52s/it]:01, 12.28 examples/s]\n",
      "Batches: 100%|██████████| 1/1 [00:02<00:00,  2.45s/it]0:58, 12.38 examples/s]\n",
      "Batches: 100%|██████████| 1/1 [00:02<00:00,  2.59s/it]0:55, 12.56 examples/s]\n",
      "Batches: 100%|██████████| 1/1 [00:02<00:00,  2.76s/it]0:53, 12.44 examples/s]\n",
      "Batches: 100%|██████████| 1/1 [00:02<00:00,  2.79s/it]0:51, 12.11 examples/s]\n",
      "Batches: 100%|██████████| 1/1 [00:02<00:00,  2.38s/it]0:50, 11.85 examples/s]\n",
      "Batches: 100%|██████████| 1/1 [00:02<00:00,  2.62s/it]0:46, 12.26 examples/s]\n",
      "Batches: 100%|██████████| 1/1 [00:02<00:00,  2.34s/it]0:43, 12.21 examples/s]\n",
      "Batches: 100%|██████████| 1/1 [00:02<00:00,  2.40s/it]0:39, 12.56 examples/s]\n",
      "Batches: 100%|██████████| 1/1 [00:02<00:00,  2.36s/it]0:36, 12.73 examples/s]\n",
      "Batches: 100%|██████████| 1/1 [00:02<00:00,  2.39s/it]0:33, 12.91 examples/s]\n",
      "Batches: 100%|██████████| 1/1 [00:02<00:00,  2.83s/it]0:31, 13.02 examples/s]\n",
      "Batches: 100%|██████████| 1/1 [00:03<00:00,  3.20s/it]0:30, 12.40 examples/s]\n",
      "Batches: 100%|██████████| 1/1 [00:03<00:00,  3.36s/it]0:29, 11.51 examples/s]\n",
      "Batches: 100%|██████████| 1/1 [00:02<00:00,  2.57s/it]0:28, 10.79 examples/s]\n",
      "Batches: 100%|██████████| 1/1 [00:02<00:00,  2.69s/it]0:24, 11.21 examples/s]\n",
      "Batches: 100%|██████████| 1/1 [00:02<00:00,  2.52s/it]0:21, 11.37 examples/s]\n",
      "Batches: 100%|██████████| 1/1 [00:02<00:00,  2.68s/it]0:18, 11.70 examples/s]\n",
      "Batches: 100%|██████████| 1/1 [00:02<00:00,  2.89s/it]0:15, 11.74 examples/s]\n",
      "Batches: 100%|██████████| 1/1 [00:02<00:00,  2.45s/it]0:12, 11.49 examples/s]\n",
      "Batches: 100%|██████████| 1/1 [00:02<00:00,  2.15s/it]0:09, 11.88 examples/s]\n",
      "Batches: 100%|██████████| 1/1 [00:02<00:00,  2.22s/it]0:06, 12.59 examples/s]\n",
      "Batches: 100%|██████████| 1/1 [00:02<00:00,  2.21s/it]0:04, 13.04 examples/s]\n",
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.57s/it]0:01, 13.41 examples/s]\n",
      "Creating Embeddings: 100%|██████████| 853/853 [01:09<00:00, 12.24 examples/s]\n"
     ]
    }
   ],
   "source": [
    "#create embeddings\n",
    "def create_embeddings(examples):\n",
    "    embeddings = model.encode(examples[\"job_description\"], show_progress_bar=True)\n",
    "    return {\"embeddings\": embeddings.tolist()}\n",
    "\n",
    "embedded_dataset = dataset.map(\n",
    "    create_embeddings,\n",
    "    batched = True,\n",
    "    batch_size = 32,\n",
    "    desc = \"Creating Embeddings\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': ['company_name', 'job_description', 'position_title', 'description_length', 'model_response', 'embeddings']}\n"
     ]
    }
   ],
   "source": [
    "print(embedded_dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#storing the embeddings in a faiss vector database\n",
    "embeddings_array = np.array(embedded_dataset[\"train\"][\"embeddings\"])\n",
    "\n",
    "#storing in faiss\n",
    "index = faiss.IndexFlatL2(384)\n",
    "index.add(embeddings_array)\n",
    "\n",
    "faiss.write_index(index, \"faiss_index.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoding the query\n",
    "query_text = \"Looking for a machine learning role\"\n",
    "query_embedding = model.encode(query_text).reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top matching job indices: [[665 179 430 828  74]]\n"
     ]
    }
   ],
   "source": [
    "index = faiss.read_index(\"faiss_index.bin\")\n",
    "\n",
    "D,I = index.search(query_embedding, k =5)\n",
    "\n",
    "print(f\"Top matching job indices: {I}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest Neighbors: [[179  13 676  62 672]]\n",
      "Distances: [[1.0211227 1.0506432 1.0827878 1.1002285 1.1348178]]\n"
     ]
    }
   ],
   "source": [
    "def search_faiss(query_text, model, index, top_k=5):\n",
    "    query_embedding = model.encode([query_text])  # Get embedding for query\n",
    "    distances, indices = index.search(query_embedding, top_k)  # Search top-k results\n",
    "    return distances, indices\n",
    "\n",
    "query = \"Software Engineer with experience in Python and AI\"\n",
    "distances, indices = search_faiss(query, model, index)\n",
    "\n",
    "print(\"Nearest Neighbors:\", indices)\n",
    "print(\"Distances:\", distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in indices[0]:  # indices is a 2D array\n",
    "    print(dataset[\"train\"][\"job_description\"][idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "faiss.write_index(index, \"faiss_index.bin\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = faiss.read_index(\"faiss_index.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to extract text from resume PDF\n",
    "\n",
    "def extract_text_from_pdf(file_path):\n",
    "    text = \"\"\n",
    "    file_type = file_path.split(\".\")[-1].lower()\n",
    "\n",
    "    with open(file_path, 'rb') as file:\n",
    "        pdf_reader = PyPDF2.PdfReader(file)\n",
    "        for page in pdf_reader.pages:\n",
    "            extracted_text = page.extract_text()\n",
    "            if extracted_text:  # Ensure text is not None\n",
    "                text += extracted_text + \"\\n\"\n",
    "    return text\n",
    "\n",
    "\n",
    "#Function to find the matching jobs\n",
    "\n",
    "\n",
    "def find_matching_jobs(resume_text, faiss_index, dataset, top_k=5):\n",
    "    # Create embedding for the resume\n",
    "    resume_embedding = model.encode([resume_text])\n",
    "    \n",
    "    # Search in FAISS index\n",
    "    distances, indices = faiss_index.search(resume_embedding, top_k)\n",
    "    \n",
    "    # Get matching job details\n",
    "    matches = []\n",
    "    for i, idx in enumerate(indices[0]):\n",
    "        # Convert numpy.int64 to standard Python int\n",
    "        idx = int(idx)\n",
    "        \n",
    "        if idx < len(dataset):  # Safety check\n",
    "            try:\n",
    "                job_title = dataset[idx]['position_title']\n",
    "                job_description = dataset[idx]['job_description']\n",
    "                \n",
    "                matches.append({\n",
    "                    'position_title': job_title,\n",
    "                    'job_description': job_description,\n",
    "                    'match_score': (1 - distances[0][i]) * 100  # Convert distance to similarity percentage\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing match {idx}: {e}\")\n",
    "    \n",
    "    return matches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Main function for gradio interface\n",
    "import tempfile\n",
    "def resume_process(file, dataset, faiss_index):\n",
    "    try:\n",
    "        # Directly use the file path provided by Gradio\n",
    "        resume_text = extract_text_from_pdf(file.name)\n",
    "\n",
    "        matches = find_matching_jobs(resume_text, faiss_index, dataset)\n",
    "\n",
    "        results = \"\"\n",
    "        for i, match in enumerate(matches):\n",
    "            results += f\"**{i+1}.{match['position_title']}**\\n\"\n",
    "            results += f\"Match score : {match['match_score']:.2f}%\\n\"\n",
    "            results += f\"Job_Description: {match['job_description'][:100000000000000000000]}...\\n\\n\"\n",
    "            results += \"---\\n\\n\"\n",
    "\n",
    "        return resume_text, results\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"Error processing file {str(e)}\",\"No matches found due to error.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "BlockContext.__init__() got an unexpected keyword argument 'align'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[99], line 33\u001b[0m\n\u001b[0;32m     30\u001b[0m faiss_index \u001b[38;5;241m=\u001b[39m faiss\u001b[38;5;241m.\u001b[39mIndexFlatL2(\u001b[38;5;241m384\u001b[39m)\n\u001b[0;32m     31\u001b[0m faiss_index\u001b[38;5;241m.\u001b[39madd(embeddings_array)\n\u001b[1;32m---> 33\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[99], line 6\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_resume_wrapper\u001b[39m(file):\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m resume_process(file, dataset, index)\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBlocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtitle\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mResume Job Matcher\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malign\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcenter\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m demo:\n\u001b[0;32m      7\u001b[0m     gr\u001b[38;5;241m.\u001b[39mMarkdown(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#Resume Job Matcher\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m     gr\u001b[38;5;241m.\u001b[39mMarkdown(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpload your resume to find matching job description\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\shind\\anaconda3\\envs\\AIres\\Lib\\site-packages\\gradio\\blocks.py:1157\u001b[0m, in \u001b[0;36mBlocks.__init__\u001b[1;34m(self, theme, analytics_enabled, mode, title, css, css_paths, js, head, head_paths, fill_height, fill_width, delete_cache, **kwargs)\u001b[0m\n\u001b[0;32m   1154\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menable_monitoring: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1156\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault_config \u001b[38;5;241m=\u001b[39m BlocksConfig(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m-> 1157\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mrender\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1159\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m=\u001b[39m mode\n\u001b[0;32m   1160\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_running \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: BlockContext.__init__() got an unexpected keyword argument 'align'"
     ]
    }
   ],
   "source": [
    "#main gradio function\n",
    "import gradio as gr\n",
    "def main():\n",
    "    def process_resume_wrapper(file):\n",
    "        return resume_process(file, dataset, index)\n",
    "    with gr.Blocks(title = \"Resume Job Matcher\") as demo:\n",
    "        gr.Markdown(\"#Resume Job Matcher\")\n",
    "        gr.Markdown(\"Upload your resume to find matching job description\")\n",
    "    \n",
    "        with gr.Row():\n",
    "            with gr.Column():\n",
    "                file_input = gr.File(label=\"Upload your pdf file\")\n",
    "                submit_btn = gr.Button(\"Find Matching Jobs\")\n",
    "            with gr.Column():\n",
    "                resume_text = gr.Textbox(label=\"Extracted resume text\", lines=20)\n",
    "                results_output = gr.Markdown(label=\"Matching Jobs\")\n",
    "        submit_btn.click(\n",
    "            fn= process_resume_wrapper,\n",
    "            inputs=[file_input],\n",
    "            outputs=[resume_text, results_output]\n",
    "        )\n",
    "    demo.launch()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    dataset_obj = load_dataset(\"jacob-hugging-face/job-descriptions\")\n",
    "    dataset = dataset_obj[\"train\"]  \n",
    "    \n",
    "    # Assuming you have precomputed embeddings\n",
    "    embeddings_array = np.array(embedded_dataset[\"train\"][\"embeddings\"])\n",
    "    faiss_index = faiss.IndexFlatL2(384)\n",
    "    faiss_index.add(embeddings_array)\n",
    "\n",
    "    main()\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#storing the embeddings in a faiss vector database\n",
    "embeddings_array = np.array(embedded_dataset[\"train\"][\"embeddings\"])\n",
    "\n",
    "#storing in faiss\n",
    "index = faiss.IndexFlatL2(384)\n",
    "index.add(embeddings_array)\n",
    "\n",
    "faiss.write_index(index, \"faiss_index.bin\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AIres",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
